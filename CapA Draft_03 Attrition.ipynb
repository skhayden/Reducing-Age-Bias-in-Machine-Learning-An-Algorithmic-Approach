{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shayden\\Anaconda3\\envs\\RPy\\lib\\site-packages\\ipykernel_launcher.py:9: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# Capstone A Project - Spring 2020\n",
    "# Kaggle Attrition data - Exploratory Data Analysis Python Code \n",
    "# Reference URL:  https://www.kaggle.com/abhishektibrewal/hr-employee-attrition\n",
    "\n",
    "# data transformation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "\n",
    "# data plotting\n",
    "import seaborn as sns\n",
    "#from matplotlib import pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# classification & correlation\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics as mt\n",
    "#import xgboost as xgb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-632838ee80d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# What version of Python do you have?\n",
    "import sys\n",
    "\n",
    "import tensorflow.keras\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tensorflow.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing data from an Excel file:\n",
    "## Prior to loading the file, convert it to csv in Excel \n",
    "## Setting datatypes prior to load\n",
    "\n",
    "## updated data types to match types in IBM's as listed here:  \n",
    "## http://inseaddataanalytics.github.io/INSEADAnalytics/groupprojects/January2018FBL/IBM_Attrition_VSS.html\n",
    "\n",
    "datatypes = {\n",
    "\"Age\": \"int\",\n",
    "\"Attrition\": \"category\",\n",
    "\"BusinessTravel\": \"category\",\n",
    "\"DailyRate\": \"int\",\n",
    "\"Department\": \"category\",\n",
    "\"DistanceFromHome\": \"int\",\n",
    "\"Education\": \"int\", ## changed to int\n",
    "\"EducationField\": \"category\",\n",
    "\"EmployeeCount\": \"int\",\n",
    "\"EmployeeNumber\": \"int\",\n",
    "\"EnvironmentSatisfaction\": \"int\", ## changed\n",
    "\"Gender\": \"category\",\n",
    "\"HourlyRate\": \"int\",\n",
    "\"JobInvolvement\": \"int\", ## changed\n",
    "\"JobLevel\": \"int\", #changed\n",
    "\"JobRole\": \"category\",\n",
    "\"JobSatisfaction\": \"int\", ## changed\n",
    "\"MaritalStatus\": \"category\",\n",
    "\"MonthlyIncome\": \"int\",\n",
    "\"MonthlyRate\": \"int\",\n",
    "\"NumCompaniesWorked\": \"int\",\n",
    "\"Over18\": \"category\",\n",
    "\"OverTime\": \"category\",\n",
    "\"PercentSalaryHike\": \"float\",\n",
    "\"PerformanceRating\": \"int\", ## changed\n",
    "\"RelationshipSatisfaction\": \"int\", ## changed\n",
    "\"StandardHours\": \"int\",\n",
    "\"StockOptionLevel\": \"int\",  ## changed\n",
    "\"TotalWorkingYears\": \"int\",\n",
    "\"TrainingTimesLastYear\": \"int\",\n",
    "\"WorkLifeBalance\": \"int\", #changed\n",
    "\"YearsAtCompany\": \"int\",\n",
    "\"YearsInCurrentRole\": \"int\",\n",
    "\"YearsSinceLastPromotion\": \"int\",\n",
    "\"YearsWithCurrManager\": \"int\"\n",
    "}\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/skhayden/SMU-Capstone-Age-Bias-in-Predictive-Modeling-/master/WA_Fn-UseC_-HR-Employee-Attrition.csv',low_memory=False)\n",
    "\n",
    "\n",
    "# Verifying the upload of the data looking at the first 5 rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Verifying the upload of the data looking at the last 5 rows\n",
    "\n",
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying the dimensions of the data set - (rows, cols)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what data types the features are\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve simple statistics from the data set\n",
    "\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring our Age feature\n",
    "\n",
    "df.Age.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring our Age feature\n",
    "\n",
    "df.Age.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decade increments\n",
    "bin_labels10 = ['<=30', '>30 & <=40', '>40 & <=50', '>50']\n",
    "bins10 = [min(df['Age']), 30, 40, 50, max(df['Age'])]\n",
    "df['Age_group_Decade'] = pd.cut(df['Age'], bins=bins10, labels=bin_labels10)\n",
    "\n",
    "\n",
    "#half decade increments\n",
    "bin_labels5 = ['<=25', '>25 & <=30','>30 & <=35','>35 & <=40','>40 & <=45','>45 & <=50','>50']\n",
    "bins5 = [min(df['Age']), 25, 30, 35, 40, 45, 50, max(df['Age'])]\n",
    "df['Age_group_half_Decade'] = pd.cut(df['Age'], bins=bins5, labels=bin_labels5)\n",
    "\n",
    "#year increments\n",
    "bin_labels1 = list(range(min(df['Age']),max(df['Age'])))\n",
    "bins1 = list(range(min(df['Age']),max(df['Age'])))\n",
    "df['Age_group_Year'] = pd.cut(df['Age'], bins=bins1, labels=bin_labels1[0:len(bin_labels1)-1])\n",
    "\n",
    "\n",
    "\n",
    "fig, ((ax0, ax1,ax2)) = plt.subplots( ncols=3,figsize=(20, 10))\n",
    "\n",
    "ax0.hist([df['Age'].loc[df['Attrition']=='Yes'],\n",
    "          df['Age']],bins=bins5, stacked=True,color=['green', 'orange'], \n",
    "          rwidth=0.98,label=['Attrition Yes', 'Attrition No'])\n",
    "ax0.set_title('5 Year Increments')\n",
    "ax0.legend(prop={'size': 10})\n",
    "\n",
    "ax1.hist([df['Age'].loc[df['Attrition']=='Yes'],\n",
    "          df['Age']],bins=bins10, stacked=True,color=['green', 'orange'], \n",
    "          rwidth=0.98,label=['Attrition Yes', 'Attrition No'])\n",
    "ax1.set_title('10 Year Increments')\n",
    "ax1.legend(prop={'size': 10})\n",
    "\n",
    "ax2.hist([df['Age'].loc[df['Attrition']=='Yes'],\n",
    "          df['Age']],bins=bins1, stacked=True,color=['green', 'orange'], \n",
    "          rwidth=0.85,label=['Attrition Yes', 'Attrition No'])\n",
    "ax2.set_title('1 Year Increments')\n",
    "ax2.legend(prop={'size': 10})\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "#10 year perc\n",
    "Agedf10=df.groupby(['Age_group_Decade', 'Attrition']).agg({'Attrition': 'count'})\n",
    "Age10_pcts = Agedf10.groupby(level=0).apply(lambda x:\n",
    "                                                 100 * x / float(x.sum()))\n",
    "#5 year perc\n",
    "Agedf5=df.groupby(['Age_group_half_Decade', 'Attrition']).agg({'Attrition': 'count'})\n",
    "Age5_pcts = Agedf5.groupby(level=0).apply(lambda x:\n",
    "                                                 100 * x / float(x.sum()))\n",
    "\n",
    "#1 year perc\n",
    "Agedf1=df.groupby(['Age_group_Year', 'Attrition']).agg({'Attrition': 'count'})\n",
    "Age1_pcts = Agedf1.groupby(level=0).apply(lambda x:\n",
    "                                                 100 * x / float(x.sum()))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Age10_pcts.loc[idx[:, ['Yes']], idx['Attrition']].plot.bar(figsize=(5, 5),layout=(2, 2),\n",
    "                title='Attrtion percentage by age group',rot=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Age5_pcts.loc[idx[:, ['Yes']], idx['Attrition']].plot.bar(figsize=(5, 5),layout=(2, 2),\n",
    "                title='Attrtion percentage by age group',rot=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Age1_pcts.loc[idx[:, ['Yes']], idx['Attrition']].plot.bar(figsize=(10, 5),\n",
    "                    layout=(2, 2),title='Attrtion percentage by age group',\n",
    "                    rot=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In order to get familiarized with data set, we are displaying categorical frequency groupings for:\n",
    "## categorical features \n",
    "## some numerical features with levels \n",
    "## target label (Y - Attrition)\n",
    "\n",
    "##### Categorical Features   #####\n",
    "\n",
    "## Attrition Frequency\n",
    "df_byAttrition = df.groupby(['Attrition'])\n",
    "print(df_byAttrition.Attrition.count())\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "## BusinessTravel Frequency\n",
    "df_byBusinessTravel = df.groupby(['BusinessTravel'])\n",
    "print(df_byBusinessTravel.BusinessTravel.count())\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "## Department Frequency\n",
    "df_byDepartment = df.groupby(['Department'])\n",
    "print(df_byDepartment.Department.count())\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "## Education Field Frequency Table\n",
    "df_byEducationField = df.groupby(['EducationField'])\n",
    "print(df_byEducationField .EducationField.count())\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "## Gender Frequency Table\n",
    "df_byGender = df.groupby(['Gender'])\n",
    "print(df_byGender.Gender.count())\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "## JobRole Frequency Table\n",
    "df_byJobRole = df.groupby(['JobRole'])\n",
    "print(df_byJobRole.JobRole.count())\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "## MaritalStatus  Frequency Table\n",
    "df_byMaritalStatus  = df.groupby(['MaritalStatus'])\n",
    "print(df_byMaritalStatus.MaritalStatus.count())\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "## Over18  Frequency Table\n",
    "df_byOver18  = df.groupby(['Over18'])\n",
    "print(df_byOver18.Over18.count())\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "## OverTime  Frequency Table\n",
    "df_byOverTime  = df.groupby(['OverTime'])\n",
    "print(df_byOverTime.OverTime.count())\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Numerical features with levels  ####\n",
    "print(\"Numerical features with levels\")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "\n",
    "\n",
    "## Education Frequency\n",
    "df_byEducation = df.groupby(['Education'])\n",
    "print(df_byEducation.Education.count())\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "## df_byEnvironmentSatisfaction Frequency Table\n",
    "df_byEnvironmentSatisfaction = df.groupby(['EnvironmentSatisfaction'])\n",
    "print(df_byEnvironmentSatisfaction.EnvironmentSatisfaction.count())\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "## JobInvolvement Frequency Table\n",
    "df_byJobInvolvement = df.groupby(['JobInvolvement'])\n",
    "print(df_byJobInvolvement.JobInvolvement.count())\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "## JobLevel Frequency Table\n",
    "df_byJobLevel = df.groupby(['JobLevel'])\n",
    "print(df_byJobLevel.JobLevel.count())\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "## JobSatisfaction Frequency Table\n",
    "df_byJobSatisfaction = df.groupby(['JobSatisfaction'])\n",
    "print(df_byJobSatisfaction.JobSatisfaction.count())\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "## PerformanceRating  Frequency Table\n",
    "df_byPerformanceRating  = df.groupby(['PerformanceRating'])\n",
    "print(df_byPerformanceRating.PerformanceRating.count())\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "## RelationshipSatisfaction  Frequency Table\n",
    "df_byRelationshipSatisfaction  = df.groupby(['RelationshipSatisfaction'])\n",
    "print(df_byRelationshipSatisfaction.RelationshipSatisfaction.count())\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "## StockOptionLevel  Frequency Table\n",
    "df_byStockOptionLevel  = df.groupby(['StockOptionLevel'])\n",
    "print(df_byStockOptionLevel.StockOptionLevel.count())\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "## WorkLifeBalance  Frequency Table\n",
    "df_byWorkLifeBalance  = df.groupby(['WorkLifeBalance'])\n",
    "print(df_byWorkLifeBalance.WorkLifeBalance.count())\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reducing levels in Education feature from 5(five) to 4 (four) levels\n",
    "\n",
    "df_byEducation = df.groupby(['Education'])\n",
    "print(df_byEducation.Education.count())\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "i = 0\n",
    "while (i < df.Education.count()):\n",
    "   ##print (\"The Education value is:\", df.Education[i])\n",
    "   if df.Education[i] == 5:\n",
    "     df.Education[i] = 4\n",
    "   i = i + 1\n",
    "\n",
    "\n",
    "df_byEducation = df.groupby(['Education'])\n",
    "print(df_byEducation.Education.count())\n",
    "print(\" \")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the target with a binary flag. 1 indicates that they left, and 0 indicates they did not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['OverTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OneHotdf.columns\n",
    "#list(df.select_dtypes(exclude = 'int64').columns)\n",
    "#Categorical_Columns= list(df.select_dtypes(exclude = 'int64').columns)\n",
    "#Categorical_Columns=Categorical_Columns[0:len(Categorical_Columns)-3]\n",
    "#Categorical_Columns\n",
    "#Categorical_Columns[0:len(Categorical_Columns)-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Steven One hot encouding \n",
    "\n",
    "exclude=df.select_dtypes('int64').columns\n",
    "\n",
    "#get columns to onehot encoude\n",
    "Numeric_Columns=list(df.select_dtypes('int64').columns)\n",
    "Categorical_Columns= list(df.select_dtypes(exclude = 'int64').columns)\n",
    "\n",
    "               \n",
    "########\n",
    "#onehot\n",
    "OneHotdf = pd.get_dummies(df[Categorical_Columns].drop(['Attrition',\n",
    "                                'Age_group_Decade','Age_group_half_Decade',\n",
    "                                'Age_group_Year'],axis=1))\n",
    "\n",
    "#remove binary varible that is duplicated\n",
    "OneHotdf = OneHotdf.drop(['OverTime_No','Gender_Male'],axis=1)\n",
    "\n",
    "#df=df.drop(Categorical_Columns,axis=1)\n",
    "df = pd.concat([df,OneHotdf], axis=1)\n",
    "\n",
    "\n",
    "#create target \n",
    "df['Target_Attrition_Flag'] = np.where(df['Attrition']=='Yes', 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing, Label Encoding Attrition, the Target Label, and non-numerical value features to add to correlation matrix and\n",
    "## AttritionOne will be used as the Target label in  XGBoster classifier\n",
    "\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "AttritionOne       = le.fit_transform(df.Attrition)\n",
    "BusinessTravelOne  = le.fit_transform(df.BusinessTravel)\n",
    "DepartmentOne      = le.fit_transform(df.Department)\n",
    "EducationFieldOne  = le.fit_transform(df.EducationField)\n",
    "GenderOne          = le.fit_transform(df.Gender)\n",
    "JobRoleOne         = le.fit_transform(df.JobRole)\n",
    "MaritalStatusOne   = le.fit_transform(df.MaritalStatus)\n",
    "Over18One          = le.fit_transform(df.Over18)\n",
    "OverTimeOne        = le.fit_transform(df.OverTime)\n",
    "\n",
    "\n",
    "\n",
    "df2 = pd.DataFrame({'AttritionOne':AttritionOne.tolist(),\n",
    "                    'BusinessTravelOne':BusinessTravelOne.tolist(),\n",
    "                    'DepartmentOne':DepartmentOne.tolist(),\n",
    "                    'EducationFieldOne':EducationFieldOne.tolist(),\n",
    "                    'GenderOne':GenderOne.tolist(),\n",
    "                    'JobRoleOne':JobRoleOne.tolist(),\n",
    "                    'MaritalStatusOne':MaritalStatusOne.tolist(),\n",
    "                    'Over18One':Over18One.tolist(),\n",
    "                    'OverTimeOne':OverTimeOne.tolist()\n",
    "                   })\n",
    "\n",
    "df3 = pd.concat([df, df2], axis=1)\n",
    "\n",
    "## print(df2.AttritionOne)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.AttritionOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore correlations between features\n",
    "\n",
    "corr = df3.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping feature correlation with a Seaborn heatmap  \n",
    "# Referenced tutorial:  https://riptutorial.com/seaborn/example/31922/basic-correlation-plot\n",
    "\n",
    "# Set background color / chart style\n",
    "sns.set_style(style = 'white')\n",
    "\n",
    "# Set up  matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Add diverging colormap\n",
    "cmap = sns.diverging_palette(10, 250, as_cmap=True)\n",
    "\n",
    "# Draw correlation plot\n",
    "sns.heatmap(corr, \n",
    "        cmap=cmap, \n",
    "        square=True,\n",
    "        linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax, \n",
    "        center=0,\n",
    "        vmin=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create group attribute for age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "df.loc[df['Age'] < 20, 'Age_Group'] = '<20'\n",
    "df.loc[(df['Age'] >=20 ) & (df['Age'] < 30), 'Age_Group'] = '>=20 & <30'\n",
    "df.loc[(df['Age'] >=30 ) & (df['Age'] < 40), 'Age_Group'] = '>=30 & <40'\n",
    "df.loc[(df['Age'] >=40 ) & (df['Age'] < 50), 'Age_Group'] = '>=40 & <50'\n",
    "df.loc[(df['Age'] >=50 ), 'Age_Group'] = '>=50'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Data and Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training and testing data set. This is also where we included and excluded \"**Age**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop Categrical columns that have dummie columns\n",
    "df=df.drop(Categorical_Columns[:-3],axis=1)\n",
    "#remove vars from trainning either for bias or target value\n",
    "# This is where you remove \"Age\" by including in the line below.\n",
    "Columns_to_drop_for_Training=['Target_Attrition_Flag','Age_group_Year', 'Age_group_Decade','Age_group_half_Decade','Age']\n",
    "\n",
    "# Split into train/test \n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(Columns_to_drop_for_Training,axis=1), df['Target_Attrition_Flag'], test_size=0.25)\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 0\n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "#tf.random.set_seed(seed_value)\n",
    "# for later versions: \n",
    "tf.compat.v1.set_random_seed(seed_value)\n",
    "\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "from keras import backend as K\n",
    "#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "#sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "#K.set_session(sess)\n",
    "# for later versions:\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.layers import LeakyReLU\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Build the neural network\n",
    "##model = Sequential()\n",
    "##model.add(Dense(25, input_dim=X_train.shape[1], activation='relu')) # Hidden 1\n",
    "##model.add(Dense(10, activation='relu')) # Hidden 2\n",
    "\n",
    "## model.add(Dense(1)) # Output\n",
    "\n",
    "##model.add(Dense(y_train.shape[1],activation='softmax')) # Output\n",
    "##model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "##model.fit(X_train,y_train,verbose=2,epochs=100)\n",
    "##mean_squared_error\n",
    "##sparse_categorical_crossentropy\n",
    "\n",
    "#del model\n",
    "\n",
    "#model = Sequential()\n",
    "#model.add(Dense(25, input_dim = X_train.shape[1], activation=LeakyReLU(alpha=0.05))) # Hidden 1\n",
    "#model.add(Dense(10, activation='relu')) # Hidden 2\n",
    "##model.add(Dense(y_train.shape[1],activation='softmax')) # Output\n",
    "#model.add(Dense(1,activation='softmax')) # Output\n",
    "#sgd = tf.keras.optimizers.SGD(lr=0.8, momentum=0.9, nesterov=True)        \n",
    "#model.compile(loss='binary_crossentropy', optimizer=sgd)\n",
    "\n",
    "## Steven's Changes\n",
    "#model.add(Dense(1,activation='sigmoid')) # Output\n",
    "#sgd = tf.keras.optimizers.SGD(lr=.0000001, decay=1e-6, momentum=0.65, nesterov=False)# Best parm so far\n",
    "##sgd = tf.keras.optimizers.SGD(lr=.00000001, decay=1e-3, momentum=0.5, nesterov=False)\n",
    "#model.compile(loss='binary_crossentropy', optimizer=sgd)\n",
    "#model.fit(X_train,y_train, verbose=2, epochs=100, batch_size=64)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## try leakyReLu\n",
    "leaky = lambda x: tf.keras.layers.LeakyReLU(alpha=0.8)(x)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim = X_train.shape[1], activation=leaky)) # Hidden 1\n",
    "\n",
    "model.add(Dense(10, activation=leaky)) # Hidden 2\n",
    "\n",
    "model.add(Dense(1,activation='sigmoid')) # Output\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(lr=.000001, decay=1e-6, momentum=0.7, nesterov=False)# Best parm so far\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd)\n",
    "\n",
    "#adds an early stop incase the model gets stuck with higher epoch counts \n",
    "#monitor = EarlyStopping(monitor='val_loss', min_delta=1e-2, patience=3, \n",
    "                        #verbose=1, mode='auto', restore_best_weights=True)\n",
    "model.fit(X_train,y_train, verbose=2, epochs=50, batch_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Break out age groups\n",
    "We want to break out the age groups and compare their accuracy amongst each other to get an understanding of bias in the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get overall accuracy \n",
    "Overall_pred = pd.DataFrame(model.predict(X_test))\n",
    "Overall_y_compare = y_test\n",
    "Overall_score = metrics.accuracy_score(Overall_y_compare, Overall_pred.astype(np.int32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add back in age group \n",
    "X_test=pd.merge(X_test, df.drop(df.columns.difference(Columns_to_drop_for_Training), 1), left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion to get accuracy accrossed groups in grouped data\n",
    "def Group_accuracy (x_test,y_test,Grouping,Atts_not_in_training,Print):\n",
    "    # This function calculates accuracy of the model and can calculate accuracy \n",
    "    #across several groups in the data. This give you information need to \n",
    "    #produce a confusion matrix \n",
    "    \n",
    "    #x_test = Test data of the explanatory variables  \n",
    "    #y_test = Test data of the target variable \n",
    "    #Grouping = The variable that contains the grouping of the \n",
    "        #data that this function will use to break out accuracy \n",
    "    #Atts_not_in_training = Attributes not in x_test that where \n",
    "        #not in x_test for training the model. \n",
    "    \n",
    "    Preds={}\n",
    "    y_compare={}\n",
    "    score={}\n",
    "    \n",
    "      \n",
    "    for group in list(x_test[Grouping].dropna().unique()):\n",
    "        \n",
    "        #Get only the actuals based on the index of the \n",
    "        # independent variables within the specified age group  \n",
    "        Preds[group]=pd.DataFrame(model.predict(x_test.loc[x_test[Grouping]==group].drop(Atts_not_in_training,axis=1)))\n",
    "        \n",
    "        #Get scores for each age group\n",
    "        y_compare[group] = pd.DataFrame(y_test[x_test.loc[x_test[Grouping] == group].index])\n",
    "        \n",
    "        #Get scores for each age group\n",
    "        score[group] = metrics.accuracy_score(y_compare[group], Preds[group].astype(np.int32))\n",
    "    \n",
    "    if Print==True:\n",
    "        \n",
    "        for key in score.keys():\n",
    "            print(\"# of obs for age group \"+ str(key) + \" :{}\".format(str(score[key])))\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy of the model: 0.8586956521739131\n",
      "Age_group_Decade\n",
      "# of obs for age group >40 & <=50 :0.9213483146067416\n",
      "# of obs for age group >30 & <=40 :0.8187919463087249\n",
      "# of obs for age group <=30 :0.8426966292134831\n",
      "# of obs for age group >50 :0.972972972972973\n",
      "Age_group_half_Decade\n",
      "# of obs for age group >45 & <=50 :0.9032258064516129\n",
      "# of obs for age group >30 & <=35 :0.75\n",
      "# of obs for age group <=25 :0.7619047619047619\n",
      "# of obs for age group >35 & <=40 :0.9180327868852459\n",
      "# of obs for age group >50 :0.972972972972973\n",
      "# of obs for age group >25 & <=30 :0.8676470588235294\n",
      "# of obs for age group >40 & <=45 :0.9310344827586207\n"
     ]
    }
   ],
   "source": [
    "#Overall accuracy \n",
    "print(\"Overall accuracy of the model: {}\".format(str(Overall_score))) \n",
    "\n",
    "#Get the predictions for only specified age groups\n",
    "print('Age_group_Decade')\n",
    "Decade_score=Group_accuracy (X_test,y_test,'Age_group_Decade',Columns_to_drop_for_Training,True)  \n",
    "print('Age_group_half_Decade')\n",
    "Half_Decade_score=Group_accuracy (X_test,y_test,'Age_group_half_Decade',Columns_to_drop_for_Training,True)\n",
    "#print('Age_group_Year')\n",
    "#Year_score=Group_accuracy (X_test,y_test,'Age_group_Year',Columns_to_drop_for_Training,True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test['Age_group_Decade'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['Age_group_half_Decade'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['Age_group_Year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
